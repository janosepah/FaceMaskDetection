{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Face_Mask_Detection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Trn7HTG9AK1Z",
        "H6esQhr7OBEO",
        "MchcRbXtG6_s"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janosepah/FaceMaskDetection/blob/main/Face_Mask_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2dHaj9uG6_W"
      },
      "source": [
        "# **Final Project: Face Mask Detector using Deep Learning** \n",
        "by Azadeh Ghaffari and Safoura Janosepah\n",
        "\n",
        "**Objective** is to build a Deep Learning model which can identify if the person is wearing a mask or not.\n",
        "\n",
        "\n",
        "The novel COVID-19 virus has forced us all to rethink how we live our everyday lives while keeping ourselves and others safe. Face masks have emerged as a simple and effective strategy for reducing the virusâ€™s threat and also, application of face mask detection system are now in high demand for transportation means, densely populated areas, residential districts, large-scale manufacturers and other enterprises to ensure safety. Therefore, the goal of this project is to develop a face mask detector using deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZPCA9ytG6_h"
      },
      "source": [
        "### Table of Content\n",
        "\n",
        "1. About Dataset\n",
        "2. Choosing Deep Learning Model\n",
        "3. Training and Evaluation of Models\n",
        "4. Conclusion\n",
        "5. Use Cases\n",
        "6. References"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIXGh0X0G6_h"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import torch\n",
        "import shutil\n",
        "import itertools\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchvision import transforms, datasets, models\n",
        "from keras.models import Sequential\n",
        "from keras.applications.vgg19 import preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "from keras.layers import Flatten, Dense, Conv2D, BatchNormalization, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.applications import EfficientNetB1, VGG19, ResNet50, InceptionV3, MobileNet, DenseNet201\n",
        "from tensorflow.keras.applications import ResNet101,ResNet152,ResNet50V2,ResNet101V2,ResNet152V2\n",
        "from os import path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Trn7HTG9AK1Z"
      },
      "source": [
        "## 1. About Dataset\n",
        "\n",
        "The main source of data we used is Kaggle. \n",
        "\n",
        "* Data is downloaded from this link\n",
        "https://www.kaggle.com/andrewmvd/face-mask-detection\n",
        "* Data1 is downloaded from this link https://www.kaggle.com/ashishjangra27/face-mask-12k-images-dataset\n",
        "* Data2 is downloaded from this link\n",
        "https://www.kaggle.com/niharika41298/withwithout-mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-F86TEbG6_i"
      },
      "source": [
        "### Displaying sample images\n",
        "Here we just show sample images that we have in data path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yvMmUXxG6_j"
      },
      "source": [
        "i = 0\n",
        "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
        "for dirname, _, filenames in os.walk('./data'):\n",
        "    for filename in filenames[:9]:\n",
        "        img = cv2.imread(os.path.join(dirname, filename))\n",
        "        if img is not None:\n",
        "            img = cv2.cvtColor(img, cv2.IMREAD_GRAYSCALE)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "            ax = list(axes.flatten())[i]\n",
        "            ax.imshow(img)\n",
        "            ax.set_title('Image ' + str(i+1))\n",
        "            ax.axis('off')\n",
        "            i += 1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eRrKg0_G6_k"
      },
      "source": [
        "Make a small group of original images and augmented images from \"withMask\" folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8C-rnn8G6_k"
      },
      "source": [
        "images = []\n",
        "augmented = []\n",
        "path = './data1/Face Mask Dataset'\n",
        "#PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"Face Mask Dataset\"\n",
        "#path = os.path.join(PROJECT_ROOT_DIR, \"data1\", CHAPTER_ID)\n",
        "\n",
        "for set_ in os.listdir(path):\n",
        "    i, j = 4, 4\n",
        "    \n",
        "    for img in os.listdir(path+'/'+set_+'/WithMask'):\n",
        "        if img[0] != 'A':\n",
        "            if i > 0:\n",
        "                images.append(path+'/'+set_+'/WithMask/'+img)\n",
        "                i -= 1\n",
        "        else:\n",
        "            if j > 0:\n",
        "                augmented.append(path+'/'+set_+'/WithMask/'+img)\n",
        "                j -= 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdq4giA9G6_k"
      },
      "source": [
        "Display the original images that was grouped in previous step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YgtIzSSG6_l"
      },
      "source": [
        "fig, axes = plt.subplots(3, 4, figsize=(15, 15))\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(hspace=-0.5)\n",
        "\n",
        "for ax in axes.flatten():\n",
        "    ax = axes.flatten()[list(axes.flatten()).index(ax)]\n",
        "    img = cv2.imread(images[list(axes.flatten()).index(ax)])\n",
        "    img = cv2.cvtColor(img, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Original Images', size=30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bnkeg7RZAK1o"
      },
      "source": [
        "#### Image Augmentation\n",
        " Data Augmentation is a technique used to expand or enlarge your dataset by using the existing data of the dataset. We apply different techniques to expand our dataset so that it will help to train our model better with a large dataset.\n",
        "\n",
        "Image Augmentation is one of the technique we can apply on an image dataset to expand our dataset so that no overfitting occurs and our model generalizes well. \n",
        "In this project we had to do this to get a better result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IqZ8hl7G6_l"
      },
      "source": [
        "fig, axes = plt.subplots(3, 4, figsize=(15, 15))\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(hspace=-0.5)\n",
        "\n",
        "for ax in axes.flatten():\n",
        "    ax = axes.flatten()[list(axes.flatten()).index(ax)]\n",
        "    img = cv2.imread(augmented[list(axes.flatten()).index(ax)])\n",
        "    img = cv2.cvtColor(img, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.suptitle('Augmented Images', size=30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udCXuO1bG6_l"
      },
      "source": [
        "### Data preprocessing\n",
        "preprocessing steps as mentioned below was applied to all the raw input images to convert them into clean versions,which could be fed to a neural network machine learning model.\n",
        "1. Resizing the input image (128 x 128)\n",
        "2. Scaling / Normalizing images \n",
        "3. Set the train and test groups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEshHsW3G6_m"
      },
      "source": [
        "path, batch_size = './data2/maskdata/maskdata', 16\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1.0/255, horizontal_flip=True, zoom_range=0.2,\n",
        "                                  shear_range=0.2)\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(path+'/train', target_size=(128, 128), \n",
        "                                               batch_size=batch_size, class_mode='categorical')\n",
        "test_generator = test_datagen.flow_from_directory(path+'/test', target_size=(128, 128), \n",
        "                                             batch_size=batch_size, class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9q13CfAG6_m"
      },
      "source": [
        "### Plot the distribution of classes\n",
        "There are two groups of withMask and WithoutMAsk. In the diagrams below you can see the ratio of these two classes in the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaSrkpf1G6_m"
      },
      "source": [
        "path = './data2/maskdata/maskdata'\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 9))\n",
        "\n",
        "for set_ in os.listdir(path):\n",
        "    counts = []\n",
        "    ax = axes[os.listdir(path).index(set_)]\n",
        "    for class_ in os.listdir(path+'/'+set_):\n",
        "        count=len(os.listdir(path+'/'+set_+'/'+class_))\n",
        "        counts.append(count)\n",
        "    ax.bar(['With Mask', 'Without Mask'], counts, color='skyblue')\n",
        "    ax.set_title(set_)\n",
        "    ax.set_xlabel('Classes')\n",
        "    ax.set_ylabel('Number of samples')\n",
        "\n",
        "plt.suptitle('Distribution of classes', size=25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6esQhr7OBEO"
      },
      "source": [
        "## 2- Chosing and Traing Deep Learning Models\n",
        "\n",
        "In this project, we try to implement multiple Deep Learning models including : \n",
        "\n",
        "\n",
        "*   ConvNet\n",
        "*   VGG19\n",
        "*   DenseNet201\n",
        "*   OpenCV\n",
        "*   MobileNet\n",
        "*   ResNet50- 6 Models\n",
        "*   YOLO5\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcKj3UbqciYb"
      },
      "source": [
        "Based on a lot of research on face mask detection, we try to build the above models and then compare the results according to the accuracy and loss criteria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYvqsD-xduVS"
      },
      "source": [
        "## 3. Training and Evaluation of Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXDXDuR5G6_n"
      },
      "source": [
        "### Detecting the face and mask using ConvNet model\n",
        "In deep learning, a convolutional neural network is a class of deep neural networks, most commonly applied to analyzing visual imagery"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgowQijbG6_n"
      },
      "source": [
        "histories = []\n",
        "for i in range(3):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    if i > 0: \n",
        "        model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.2))\n",
        "    \n",
        "        if i > 1: \n",
        "            model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', input_shape=(128, 128, 3)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "            model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(2, activation='sigmoid'))\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
        "    histories.append(model.fit_generator(generator=train_generator, \n",
        "                                         validation_data=test_generator, \n",
        "                                         steps_per_epoch=len(train_generator)//3, \n",
        "                                         validation_steps=len(test_generator)//3, \n",
        "                                         epochs=10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQm6gFo3G6_n"
      },
      "source": [
        "#### Plot Loss and Accuracy \n",
        "plot the result of Loss and accuracy for all three layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUGToL4pG6_o"
      },
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "for metric in histories[0].history:\n",
        "    index = list(histories[0].history).index(metric)\n",
        "    ax = axes.flatten()[index]\n",
        "    layer_num = 0\n",
        "    for history in histories:\n",
        "        layer_num += 1\n",
        "        ax.plot(history.history[metric], label=str(layer_num)+' layer(s)')\n",
        "    ax.set_title(metric)\n",
        "    ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oOKI1RpG6_o"
      },
      "source": [
        "### Detecting the face and mask using OpenCV model\n",
        "OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Being a BSD-licensed product, OpenCV makes it easy for businesses to utilize and modify the code.\n",
        "using openCV to detect the face in images , we use haarcascade for that and check a sample image to see the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKPdV-iIG6_o"
      },
      "source": [
        "faceCascade = cv2.CascadeClassifier('./haarcascade/haarcascade_frontalface_default.xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiLotVcsG6_o"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#trying it out on a sample image\n",
        "img = cv2.imread('./data/meWithMask.png')\n",
        "\n",
        "gray = cv2.cvtColor(img, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "faces = faceCascade.detectMultiScale(gray,1.3,5) #returns a list of (x,y,w,h) tuples\n",
        "\n",
        "out_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) #colored output image\n",
        "\n",
        "#plotting\n",
        "for (x,y,w,h) in faces:\n",
        "    cv2.rectangle(out_img,(x,y),(x+w,y+h),(0,0,255),1)\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(out_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnGiziP7G6_o"
      },
      "source": [
        "Define a method to test all images with the trained models and test it with sample image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYcjc-tEG6_p"
      },
      "source": [
        "sample = \"./data/mewithoutMask.png\"\n",
        "\n",
        "def checkImages(model,image):\n",
        "    plt.figure(figsize=(8,7))\n",
        "    label = {0:\"With Mask\",1:\"Without Mask\"}\n",
        "    color_label = {0: (0,255,0),1 : (0,0,255)}\n",
        "    count = 0\n",
        "    frame =cv2.imread(image)\n",
        "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
        "    faces =faceCascade.detectMultiScale(gray,1.3,5)\n",
        "    for x,y,w,h in faces:\n",
        "        face_image = frame[y:y+h,x:x+w]\n",
        "        resize_img  = cv2.resize(face_image,(128,128))\n",
        "        normalized = resize_img/255.0\n",
        "        reshape = np.reshape(normalized,(1,128,128,3))\n",
        "        reshape = np.vstack([reshape])\n",
        "        result = model.predict_classes(reshape)\n",
        "\n",
        "        if result == 0:\n",
        "            cv2.rectangle(frame,(x,y),(x+w,y+h),color_label[0],3)\n",
        "            cv2.rectangle(frame,(x,y-50),(x+w,y),color_label[0],-1)\n",
        "            cv2.putText(frame,label[0],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
        "            frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "            plt.imshow(frame)\n",
        "        elif result == 1:\n",
        "            cv2.rectangle(frame,(x,y),(x+w,y+h),color_label[1],3)\n",
        "            cv2.rectangle(frame,(x,y-50),(x+w,y),color_label[1],-1)\n",
        "            cv2.putText(frame,label[1],(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2)\n",
        "            frame = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "            plt.imshow(frame)\n",
        "        #plt.imshow(frame)\n",
        "    plt.show()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "checkImages(model,sample)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSRTHjr1G6_p"
      },
      "source": [
        "### Comparing model performance in 11 models \n",
        "### (ConvNet,VGG19,DenseNet201,OpenCV, and MobileNet, ResNet)\n",
        "* Using Inception V3 : is a convolutional neural network for assisting in image analysis and object detection, and got its start as a module for Googlenet. It is the third edition of Google's Inception Convolutional Neural Network, originally introduced during the ImageNet Recognition Challenge.\n",
        "* MobileNet : are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks\n",
        "* DenseNet201: is a convolutional neural network that is 201 layers deep\n",
        "* VGG19 :  is a variant of VGG model which in short consists of 19 layers (16 convolution layers, 3 Fully connected layer, 5 MaxPool layers and 1 SoftMax layer)\n",
        "* Residual Convolutional Neural Networks 50 :\n",
        "ResNet is one of the usefull models for image recognition tasks. Most noticable feature of this neural network architecture is some direct connections which skip middle layers rather then subsequent layer (it might vary on different ResNet models). In Keras API, there are 6 different ResNet architectures (ResNet50,ResNet101,ResNet152,ResNet50V2,ResNet101V2,ResNet152V2) which we are going to use them with Transfer Learning method. For more information about how to use those pre-build ResNet models in Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-M4a2thG6_p",
        "scrolled": true
      },
      "source": [
        "model_histories = []\n",
        "all_models=[]\n",
        "height = 128\n",
        "width = 128\n",
        "models = [InceptionV3(include_top=False, input_shape=(height, width, 3)), \n",
        "                   MobileNet(include_top=False, input_shape=(height, width, 3)), \n",
        "                   DenseNet201(include_top=False, input_shape=(height, width, 3)),\n",
        "                   VGG19(include_top=False, input_shape=(height, width, 3)),ResNet50(include_top=False,input_shape=(height, width,3)),\n",
        "                    ResNet101(include_top=False, input_shape=(height, width, 3)), ResNet152(include_top=False, input_shape=(height, width, 3)),\n",
        "                    ResNet50V2(include_top=False, input_shape=(height, width, 3)),ResNet101V2(include_top=False, input_shape=(height, width, 3)),\n",
        "                    ResNet152V2(include_top=False, input_shape=(height, width, 3))]\n",
        "names = ['ConvNet', 'InceptionV3', 'MobileNet', 'DenseNet', 'VGG19', 'ResNet50','ResNet101','ResNet152','ResNet50V2','ResNet101V2','ResNet152V2']\n",
        "for layer in [Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(height, width, 3))]:\n",
        "    model = Sequential()\n",
        "    model.add(layer)\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(2, activation='sigmoid'))\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
        "    all_models.append(model)\n",
        "    model_histories.append(model.fit_generator(generator=train_generator, \n",
        "                                         validation_data=test_generator, \n",
        "                                         steps_per_epoch=len(train_generator)//3, \n",
        "                                         validation_steps=len(test_generator)//3, \n",
        "                                         epochs=10))\n",
        "\n",
        "for functional in models:\n",
        "    \n",
        "    for layer in functional.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(functional)\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(2, activation='sigmoid'))\n",
        "    model.summary()\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics='accuracy')\n",
        "    all_models.append(model)\n",
        "    model_histories.append(model.fit_generator(generator=train_generator, \n",
        "                                         validation_data=test_generator, \n",
        "                        steps_per_epoch=len(train_generator)//3, \n",
        "                        validation_steps=len(test_generator)//3, epochs=10))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXqGv59dG6_q"
      },
      "source": [
        "### Plot Loss / Accuracy \n",
        "Plot loss and accuracy for all five models over the epochs and compare them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2b3bMpAG6_q"
      },
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "fig.subplots_adjust(hspace=0.3)\n",
        "for metric in model_histories[0].history:\n",
        "    index = list(model_histories[0].history).index(metric)\n",
        "    ax = axes.flatten()[index]\n",
        "    name_index = 0\n",
        "    for history in model_histories:\n",
        "        ax.plot(history.history[metric], label=names[name_index])\n",
        "        name_index += 1\n",
        "    ax.set_title(metric+' over epochs', size=15)\n",
        "    ax.set_xlabel('epochs')\n",
        "    ax.set_ylabel(metric)\n",
        "    ax.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UvQywYKG6_r"
      },
      "source": [
        "## Result\n",
        "Having looked at the results:\n",
        "* We can see after apoch 7 we got high accurcy for most of the models and low loss. \n",
        "* MobileNet , RestNet50V2 , DenseNet, VGG19 ,RestNet152V2 have the best accuracy and loss .\n",
        "* ConvNet, RestNet101V2, RestNet152 and RestNet50 are in different level and they don't have a good result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYU5UYTdG6_s"
      },
      "source": [
        "## Test the models\n",
        "* Test the last model with new data , try to detect mask on face\n",
        "* Test models with another source of data to get the accuracy based on detection\n",
        "\n",
        "#### Based on the result that we got after training we want to run the test on the top models \n",
        "* VGG19\n",
        "* RestNet152V2\n",
        "* RestNet50V2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vmz1nCMMTVos"
      },
      "source": [
        "import sys\n",
        "test_dir=\"./data2/maskdata/maskdata/test\"\n",
        "def test_model(model):\n",
        "    total_comparison=0\n",
        "    total_incorrect=0\n",
        "    class_list=list(train_generator.class_indices.keys())\n",
        "    print(class_list[0])\n",
        "    directory_test = test_dir\n",
        "    for pic in os.listdir(directory_test):\n",
        "        dir = os.path.join(directory_test,pic)\n",
        "        for image_name in os.listdir(dir):\n",
        "            path = os.path.join(dir,image_name)\n",
        "            image = cv2.imread(path,cv2.IMREAD_COLOR)\n",
        "            image = cv2.resize(image, (128,128))\n",
        "            image = np.expand_dims(image, axis=0)\n",
        "            total_comparison+=1\n",
        "            pred_class=class_list[np.argmax(model.predict(image))]\n",
        "            if pic!=pred_class:\n",
        "                total_incorrect+=1\n",
        "                sys.stdout.write('img= ' + image_name +' total_comparisons= ' + str(total_comparison) + '  total_incorrect: ' + str(total_incorrect) + '\\r')\n",
        "                sys.stdout.flush()\n",
        "                print('')\n",
        "                print('picture=' + pic + ' but model predicted='+pred_class)\n",
        "                image = cv2.imread(path,cv2.IMREAD_COLOR)\n",
        "                image = cv2.resize(image, (128,128))\n",
        "                plt.imshow(image)\n",
        "                plt.show()\n",
        "            sys.stdout.write('img= ' + image_name +' total_comparisons= ' + str(total_comparison) + '  total_incorrect: ' + str(total_incorrect) + '\\r')\n",
        "            sys.stdout.flush()\n",
        "    print('')\n",
        "    print(\"Total comparisons=\"+str(total_comparison)+ \" incorrect pred=\"+str(total_incorrect)+\" Accuracy%=\"+str((total_comparison-total_incorrect)/total_comparison))\n",
        "    return((total_comparison-total_incorrect)/total_comparison)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KWjwYfq4TVot"
      },
      "source": [
        "imagePath = \"./data\"\n",
        "i = 0\n",
        "for myModel in all_models:\n",
        "    print(names[i] + \" : \" + str(myModel))\n",
        "    i = i + 1\n",
        "    if (i == 5 or i == 11):\n",
        "        for image_name in os.listdir(imagePath):\n",
        "            print(image_name)\n",
        "            image_path = os.path.join(imagePath,image_name)\n",
        "            checkImages(myModel,image_path)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg3RFEmKTVot"
      },
      "source": [
        "By Using OpenCV on new data we can see that \"RestNet152V2\" has the good result "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBWy1AtqTVou"
      },
      "source": [
        "i = 0\n",
        "for myModel in all_models:\n",
        "    print(names[i] + \" : \" + str(myModel))\n",
        "    i = i + 1\n",
        "    if (i == 9 or i == 5) :\n",
        "        test_model(myModel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGVQ82luTVou"
      },
      "source": [
        "## Test Result\n",
        "Based on the test that we run on \"RestNet50V2\"we got %63 accuracy and for \"VGG19\" we got %94 acuuracy \n",
        "* based on our test result we can say that we have a good train model with VGG19  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS1oS-19TVov"
      },
      "source": [
        "## 4. Conclusion\n",
        "As the technology are blooming with emerging trends the availability so we have novel face mask detector which can possibly contribute to public healthcare. The architecture consists of MobileNet as the backbone it can be used for high and low computation scenarios. In order to extract more robust features, we utilize transfer learning to adoptweights from a similar task face detection, which is trained on a very large dataset.\n",
        "We used OpenCV, tensor flow, keras , Pytorch and CNN to detect whether people were wearing face masks or not. The models were tested with images. The accuracy of the model is achieved and, the optimization ofthe model is a continuous process and we are building a highly accurate solution by tuning the hyper parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iDoDJKdTVov"
      },
      "source": [
        "## 5. Use Cases\n",
        "Here are a few use cases where this mask detection technology could be leveraged.\n",
        "\n",
        "* Airports: \n",
        "The Face Mask Detection System could be used at airports to detect travelers without masks. Face data of travelers can be captured in the system at the entrance. If a traveler is found to be without a face mask, their picture is sent to the airport authorities so that they could take quick action.\n",
        "* Hospitals: \n",
        "Using Face Mask Detector System, Hospitals can monitor if quarantined people required to wear a mask are doing so or not. The same holds good for monitoring staff on duty too.\n",
        "* Offices & Working Spaces: \n",
        "The Face Mask Detection System can be used at office premises to ascertain if employees are maintaining safety standards at work. It monitors employees without masks and sends them a reminder to wear a mask.\n",
        "* Government: \n",
        "To limit the spread of coronavirus, the police could deploy the face mask detector on its fleet of surveillance cameras to enforce the compulsory wearing of face masks in public places.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9baUi5iJTVow"
      },
      "source": [
        "## 6. Refrences\n",
        "* Kaggle\n",
        "    * https://www.kaggle.com/andrewmvd/face-mask-detection\n",
        "    * https://www.kaggle.com/ashishjangra27/face-mask-12k-images-dataset\n",
        "    * https://www.kaggle.com/niharika41298/withwithout-mask\n",
        "* https://www.irjet.net/archives/V7/i8/IRJET-V7I8530.pdf \n",
        "* https://www.ideas2it.com/blogs/face-mask-detector-using-deep-learning-pytorch-and-computer-vision-opencv/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RTMllyLG6_s"
      },
      "source": [
        "#### Why YOLO?\n",
        "Using an object detection model such as YOLOv5 is most likely the simplest and most reasonable approach to this problem. This is because weâ€™re limiting the computer vision pipeline to a single step, since object detectors are trained to detect a:\n",
        "\n",
        "*   Bounding box and a\n",
        "*   Corresponding label\n",
        "\n",
        "This is precisely what weâ€™re trying to achieve for this problem. In our case, the bounding boxes will be the detected faces, and the corresponding labels will indicate whether the person is wearing a mask or not.\n",
        "Alternatively, if we wanted to build our own deep learning model, it would be more complex, since it would have to be 2-fold: weâ€™d need a model to detect faces in an image, and a second model to detect the presence or absence of face mask in the found bounding boxes.\n",
        "\n",
        "A drawback of doing so, apart from the complexity, is that the inference time would be much slower, especially in images with many faces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIayymPpfSCY"
      },
      "source": [
        "#### Training on custom data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVCyrfpTjtga"
      },
      "source": [
        "from pathlib import Path\n",
        "from xml.dom.minidom import parse\n",
        "from shutil import copyfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHN3bllpfUoQ"
      },
      "source": [
        "#### Project layout\n",
        "The first thing we need to do is clone the repository from ultralytics/yolov5, and install all required dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "anYEHuMP_jb7"
      },
      "source": [
        "!git clone https://github.com/rkuo2000/yolov5\n",
        "%cd yolov5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sto3MSglfoWW"
      },
      "source": [
        "#### Repro YOLOv5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gzw6IQ5y_jb-"
      },
      "source": [
        "!mkdir -p Dataset/FaceMask/Images\n",
        "!mkdir -p Dataset/FaceMask/Labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF9kbk4Qf1at"
      },
      "source": [
        "!cp -rf /content/drive/MyDrive/FinalProject/MaskDetestion/data3/images/* Dataset/FaceMask/Images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiDJYnBY_jb_"
      },
      "source": [
        "!mkdir -p Dataset/images Dataset/labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEYlBxgj_jcA"
      },
      "source": [
        "#### Create Test and Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MI93fY5_jcB"
      },
      "source": [
        "FILE_ROOT = \"/content/\"\n",
        "IMAGE_PATH = FILE_ROOT + \"images\"  \n",
        "ANNOTATIONS_PATH = FILE_ROOT + \"annotations\"\n",
        "\n",
        "DATA_ROOT = \"Dataset/\"\n",
        "LABELS_ROOT = DATA_ROOT + \"FaceMask/Labels\"\n",
        "IMAGES_ROOT = DATA_ROOT + \"FaceMask/Images\"  \n",
        "\n",
        "DEST_IMAGES_PATH = \"images\"\n",
        "DEST_LABELS_PATH = \"labels\" "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgerKFxG_jcC"
      },
      "source": [
        "classes = ['with_mask', 'without_mask', 'mask_weared_incorrect']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa1XJBQFj3dy"
      },
      "source": [
        "#### Converting annotations (from COCO .xml to YOLO format .txt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP4ey2FM_jcD"
      },
      "source": [
        "def cord_converter(size, box):\n",
        "    \"\"\"\n",
        "    convert xml annotation to darknet format coordinates\n",
        "    :param sizeï¼š [w,h]\n",
        "    :param box: anchor box coordinates [upper-left x,uppler-left y,lower-right x, lower-right y]\n",
        "    :return: converted [x,y,w,h]\n",
        "    \"\"\"\n",
        "    x1 = int(box[0])\n",
        "    y1 = int(box[1])\n",
        "    x2 = int(box[2])\n",
        "    y2 = int(box[3])\n",
        "\n",
        "    dw = np.float32(1. / int(size[0]))\n",
        "    dh = np.float32(1. / int(size[1]))\n",
        "\n",
        "    w = x2 - x1\n",
        "    h = y2 - y1\n",
        "    x = x1 + (w / 2)\n",
        "    y = y1 + (h / 2)\n",
        "\n",
        "    x = x * dw\n",
        "    w = w * dw\n",
        "    y = y * dh\n",
        "    h = h * dh\n",
        "    return [x, y, w, h]\n",
        "\n",
        "def save_file(img_jpg_file_name, size, img_box):\n",
        "    save_file_name = LABELS_ROOT + '/' + img_jpg_file_name + '.txt'\n",
        "    print(save_file_name)\n",
        "    file_path = open(save_file_name, \"a+\")\n",
        "    for box in img_box:\n",
        "\n",
        "        cls_num = classes.index(box[0])\n",
        "\n",
        "        new_box = cord_converter(size, box[1:])\n",
        "\n",
        "        file_path.write(f\"{cls_num} {new_box[0]} {new_box[1]} {new_box[2]} {new_box[3]}\\n\")\n",
        "\n",
        "    file_path.flush()\n",
        "    file_path.close()\n",
        "    \n",
        "def get_xml_data(file_path, img_xml_file):\n",
        "    img_path = file_path + '/' + img_xml_file + '.xml'\n",
        "    print(img_path)\n",
        "\n",
        "    dom = parse(img_path)\n",
        "    root = dom.documentElement\n",
        "    img_name = root.getElementsByTagName(\"filename\")[0].childNodes[0].data\n",
        "    img_size = root.getElementsByTagName(\"size\")[0]\n",
        "    objects = root.getElementsByTagName(\"object\")\n",
        "    img_w = img_size.getElementsByTagName(\"width\")[0].childNodes[0].data\n",
        "    img_h = img_size.getElementsByTagName(\"height\")[0].childNodes[0].data\n",
        "    img_c = img_size.getElementsByTagName(\"depth\")[0].childNodes[0].data\n",
        "    # print(\"img_name:\", img_name)\n",
        "    # print(\"image_info:(w,h,c)\", img_w, img_h, img_c)\n",
        "    img_box = []\n",
        "    for box in objects:\n",
        "        cls_name = box.getElementsByTagName(\"name\")[0].childNodes[0].data\n",
        "        x1 = int(box.getElementsByTagName(\"xmin\")[0].childNodes[0].data)\n",
        "        y1 = int(box.getElementsByTagName(\"ymin\")[0].childNodes[0].data)\n",
        "        x2 = int(box.getElementsByTagName(\"xmax\")[0].childNodes[0].data)\n",
        "        y2 = int(box.getElementsByTagName(\"ymax\")[0].childNodes[0].data)\n",
        "        # print(\"box:(c,xmin,ymin,xmax,ymax)\", cls_name, x1, y1, x2, y2)\n",
        "        img_jpg_file_name = img_xml_file + '.jpg'\n",
        "        img_box.append([cls_name, x1, y1, x2, y2])\n",
        "    # print(img_box)\n",
        "\n",
        "    # test_dataset_box_feature(img_jpg_file_name, img_box)\n",
        "    save_file(img_xml_file, [img_w, img_h], img_box)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv0RipKo_jcF"
      },
      "source": [
        "files = os.listdir(ANNOTATIONS_PATH)\n",
        "for file in files:\n",
        "    print(\"file name: \", file)\n",
        "    file_xml = file.split(\".\")\n",
        "    get_xml_data(ANNOTATIONS_PATH, file_xml[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWmRxiLkkbEQ"
      },
      "source": [
        "#### Spliting Images dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA8CqrvJ_jcH"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "image_list = os.listdir('Dataset/FaceMask/Images')\n",
        "train_list, test_list = train_test_split(image_list, test_size=0.2, random_state=7)\n",
        "val_list, test_list = train_test_split(test_list, test_size=0.5, random_state=8)\n",
        "\n",
        "print('total =',len(image_list))\n",
        "print('train :',len(train_list))\n",
        "print('val   :',len(val_list))\n",
        "print('test  :',len(test_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOVeL928nYxk"
      },
      "source": [
        "In order to train the model, a necessary step will be to change the format of the .xml annotation files so that they conform with the darknet format. In the linked github thread, weâ€™ll see that each image has to have a .txt file associated with it, with rows with the format:\n",
        "<object-class> <x> <y> <width> <height>\n",
        "Each line will represent the annotation for each object in the image, where <x> <y> are the coordinates of the centre of the bounding box, and <width> <height> the respective width and height.\n",
        "For example an img1.jpg must have an associated img1.txt containing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59y78fZa_jcH"
      },
      "source": [
        "def copy_data(file_list, img_labels_root, imgs_source, type):\n",
        "\n",
        "    root_file = Path(DATA_ROOT + DEST_IMAGES_PATH + '/' + type)\n",
        "    if not root_file.exists():\n",
        "        print(f\"Path {root_file} is not exit\")\n",
        "        os.makedirs(root_file)\n",
        "\n",
        "    root_file = Path(DATA_ROOT + DEST_LABELS_PATH + '/' + type)\n",
        "    if not root_file.exists():\n",
        "        print(f\"Path {root_file} is not exit\")\n",
        "        os.makedirs(root_file)\n",
        "\n",
        "    for file in file_list:\n",
        "        img_name = file.replace('.png', '')\n",
        "        img_src_file = imgs_source + '/' + img_name + '.png'\n",
        "        label_src_file = img_labels_root + '/' + img_name + '.txt'\n",
        "\n",
        "        # print(img_sor_file)\n",
        "        # print(label_sor_file)\n",
        "        # im = Image.open(rf\"{img_sor_file}\")\n",
        "        # im.show()\n",
        "\n",
        "        # Copy image\n",
        "        DICT_DIR = DATA_ROOT + DEST_IMAGES_PATH + '/' + type\n",
        "        img_dict_file = DICT_DIR + '/' + img_name + '.png'\n",
        "\n",
        "        copyfile(img_src_file, img_dict_file)\n",
        "\n",
        "        # Copy label\n",
        "        DICT_DIR = DATA_ROOT + DEST_LABELS_PATH + '/' + type\n",
        "        img_dict_file = DICT_DIR + '/' + img_name + '.txt'\n",
        "        copyfile(label_src_file, img_dict_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyqIpLYV_jcI"
      },
      "source": [
        "copy_data(train_list, LABELS_ROOT, IMAGES_ROOT, \"train\")\n",
        "copy_data(val_list,   LABELS_ROOT, IMAGES_ROOT, \"val\")\n",
        "copy_data(test_list,  LABELS_ROOT, IMAGES_ROOT, \"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdqpeE6TksS2"
      },
      "source": [
        "#### Creating data/facemask.yaml"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDOXYhf7_jcI"
      },
      "source": [
        "!echo \"train: Dataset/images/train\" > data/facemask.yaml\n",
        "!echo \"val:   Dataset/images/val\" >> data/facemask.yaml\n",
        "!echo \"nc : 3\" >> data/facemask.yaml\n",
        "!echo \"names: ['With_Mask', 'Without_Mask', 'Incorrect_Mask']\" >> data/facemask.yaml\n",
        "\n",
        "!cat data/facemask.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v_eqKph_jcJ"
      },
      "source": [
        "#### Training YOLOv5\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjFN7bJ_Bhtp"
      },
      "source": [
        "!pip install PyYAML==5.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb-U9I9UE7lW"
      },
      "source": [
        "!pip install torchvision==0.7.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjtvueWk_jcJ"
      },
      "source": [
        "# Train with default Yolov5.weight\n",
        "!python train.py --img 320 --batch 16 --epochs 50 --data data/facemask.yaml --cfg models/yolov5s.yaml --weights yolov5s.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1de8_oyF_jcJ"
      },
      "source": [
        "# save trained weights for detection\n",
        "!cp runs/train/exp2/weights/best.pt weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zmaiFNylfku"
      },
      "source": [
        "#### Testing YOLOv5\n",
        "Detecting the facemask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Izml6Ea_jcK"
      },
      "source": [
        "!python detect.py --source Dataset/images/test --img-size 320 --conf 0.4 --weights weights/best.pt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH0wPwz7lwpB"
      },
      "source": [
        "#### Display detected images\n",
        "As soon as the first epoch is complete we will have a mosaic of images showing both the ground truth and prediction results on test images, which will look like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O5B07Jo_jcL"
      },
      "source": [
        "# display detected images\n",
        "from IPython.display import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gej-wEO_jcM"
      },
      "source": [
        "!python detect.py --source /content/drive/MyDrive/FinalProject/MaskDetestion/data3/images/maksssksksss103.png --img-size 320 --conf 0.4 --weights weights/best.pt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSXZAmBW_jcM"
      },
      "source": [
        "Image('runs/detect/exp2/maksssksksss103.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYc03UJj_jcN"
      },
      "source": [
        "!python detect.py --source /content/drive/MyDrive/FinalProject/MaskDetestion/data3/images/maksssksksss130.png --img-size 320 --conf 0.4 --weights weights/best.pt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McQ4gyK4_jcN"
      },
      "source": [
        "Image('runs/detect/exp5/maksssksksss130.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hodcxnI2omlz"
      },
      "source": [
        "All in all Yolo5 is one of the best model to detect object correctly. In our project we use this model to check the result and how it works, all of code we use from Kaggle and  https://medium.com/. "
      ]
    }
  ]
}